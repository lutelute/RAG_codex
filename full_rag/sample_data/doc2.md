# Local LLMs

Local models can be served with llama.cpp or llama-cpp-python.
Quantized GGUF models run on CPU and are practical for demos.
Use smaller models for speed and predictable latency.
